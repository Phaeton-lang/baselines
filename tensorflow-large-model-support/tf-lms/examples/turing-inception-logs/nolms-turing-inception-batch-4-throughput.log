2022-02-20 13:30:51.573044: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2022-02-20 13:30:52.052785: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.7
2022-02-20 13:30:52.053614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.7
2022-02-20 13:30:52.470805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-02-20 13:30:52.938816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:30:52.939086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2060 computeCapability: 7.5
coreClock: 1.68GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 312.97GiB/s
2022-02-20 13:30:52.939104: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2022-02-20 13:30:52.939127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2022-02-20 13:30:52.940329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2022-02-20 13:30:52.940530: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2022-02-20 13:30:52.941887: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2022-02-20 13:30:52.942606: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2022-02-20 13:30:52.942632: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-02-20 13:30:52.942694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:30:52.942980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:30:52.943209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2022-02-20 13:30:52.958688: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2022-02-20 13:30:52.963039: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2899885000 Hz
2022-02-20 13:30:52.963272: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563c1f797930 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-02-20 13:30:52.963286: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-02-20 13:30:52.963429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:30:52.963704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2060 computeCapability: 7.5
coreClock: 1.68GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 312.97GiB/s
2022-02-20 13:30:52.963724: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2022-02-20 13:30:52.963749: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2022-02-20 13:30:52.963760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2022-02-20 13:30:52.963769: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2022-02-20 13:30:52.963777: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2022-02-20 13:30:52.963785: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2022-02-20 13:30:52.963792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-02-20 13:30:52.963827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:30:52.964141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:30:52.964387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2022-02-20 13:30:52.964405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2022-02-20 13:30:53.426914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-20 13:30:53.426942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 
2022-02-20 13:30:53.426947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N 
2022-02-20 13:30:53.427070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:30:53.427364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:30:53.427634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:30:53.427895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5342 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5)
2022-02-20 13:30:53.429224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563c41a0b020 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-02-20 13:30:53.429235: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2060, Compute Capability 7.5
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
2022-02-20 13:31:02.683086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2022-02-20 13:31:02.870481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-02-20 13:31:04.524975: W tensorflow/core/common_runtime/bfc_allocator.cc:311] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.
2022-02-20 13:31:04.576088: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 3.72G (3991994368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
Train for 10 steps
Epoch 1/4
 1/10 [==>...........................] - ETA: 1:34 - loss: 2.9642 2/10 [=====>........................] - ETA: 42s - loss: 4.7714  3/10 [========>.....................] - ETA: 24s - loss: 5.0670 4/10 [===========>..................] - ETA: 16s - loss: 5.1601 5/10 [==============>...............] - ETA: 10s - loss: 4.6395 6/10 [=================>............] - ETA: 7s - loss: 4.7240  7/10 [====================>.........] - ETA: 4s - loss: 5.0593 8/10 [=======================>......] - ETA: 2s - loss: 5.0701 9/10 [==========================>...] - ETA: 1s - loss: 5.256510/10 [==============================] - 11s 1s/step - loss: 5.0637
Epoch 2/4
 1/10 [==>...........................] - ETA: 0s - loss: 4.1118 2/10 [=====>........................] - ETA: 0s - loss: 3.3222 3/10 [========>.....................] - ETA: 0s - loss: 4.3643 4/10 [===========>..................] - ETA: 0s - loss: 4.3731 5/10 [==============>...............] - ETA: 0s - loss: 3.8787 6/10 [=================>............] - ETA: 0s - loss: 3.9696 7/10 [====================>.........] - ETA: 0s - loss: 3.8480 8/10 [=======================>......] - ETA: 0s - loss: 4.0119 9/10 [==========================>...] - ETA: 0s - loss: 4.138310/10 [==============================] - 1s 77ms/step - loss: 4.0817
Epoch 3/4
 1/10 [==>...........................] - ETA: 0s - loss: 2.6695 2/10 [=====>........................] - ETA: 0s - loss: 3.5089 3/10 [========>.....................] - ETA: 0s - loss: 3.2621 4/10 [===========>..................] - ETA: 0s - loss: 3.1311 5/10 [==============>...............] - ETA: 0s - loss: 3.2388 6/10 [=================>............] - ETA: 0s - loss: 3.2443 7/10 [====================>.........] - ETA: 0s - loss: 3.2576 8/10 [=======================>......] - ETA: 0s - loss: 3.1611 9/10 [==========================>...] - ETA: 0s - loss: 3.060410/10 [==============================] - 1s 76ms/step - loss: 3.0217
Epoch 4/4
 1/10 [==>...........................] - ETA: 0s - loss: 1.8975 2/10 [=====>........................] - ETA: 0s - loss: 2.6710 3/10 [========>.....................] - ETA: 0s - loss: 2.5675 4/10 [===========>..................] - ETA: 0s - loss: 2.6293 5/10 [==============>...............] - ETA: 0s - loss: 2.5622 6/10 [=================>............] - ETA: 0s - loss: 2.6884 7/10 [====================>.........] - ETA: 0s - loss: 2.8208 8/10 [=======================>......] - ETA: 0s - loss: 2.6601 9/10 [==========================>...] - ETA: 0s - loss: 2.704610/10 [==============================] - 1s 78ms/step - loss: 2.7130
2022-02-20 13:31:09.099296: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
training throughput: 11.701894244130768
peak active bytes(MB): 2702.2392578125
