2022-02-20 13:44:09.449352: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2022-02-20 13:44:09.923826: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.7
2022-02-20 13:44:09.924651: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.7
2022-02-20 13:44:10.338681: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-02-20 13:44:10.806180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:44:10.806469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2060 computeCapability: 7.5
coreClock: 1.68GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 312.97GiB/s
2022-02-20 13:44:10.806486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2022-02-20 13:44:10.806511: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2022-02-20 13:44:10.807775: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2022-02-20 13:44:10.807992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2022-02-20 13:44:10.809465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2022-02-20 13:44:10.810246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2022-02-20 13:44:10.810274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-02-20 13:44:10.810339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:44:10.810647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:44:10.810892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2022-02-20 13:44:10.819801: E bazel-out/k8-py2-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc:5187] (GetBFCAllocatorStats) No GPU device registered. Skipping getting stats

2022-02-20 13:44:10.819820: E bazel-out/k8-py2-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc:5388] (getPeakBytesActive) - Could not retrieve BFC Allocator Stats
2022-02-20 13:44:10.827492: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2022-02-20 13:44:10.831827: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2899885000 Hz
2022-02-20 13:44:10.831995: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558392b57920 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-02-20 13:44:10.832031: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-02-20 13:44:10.832160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:44:10.832435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2060 computeCapability: 7.5
coreClock: 1.68GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 312.97GiB/s
2022-02-20 13:44:10.832468: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2022-02-20 13:44:10.832478: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2022-02-20 13:44:10.832489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2022-02-20 13:44:10.832498: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2022-02-20 13:44:10.832505: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2022-02-20 13:44:10.832513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2022-02-20 13:44:10.832519: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-02-20 13:44:10.832555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:44:10.832896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:44:10.833140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2022-02-20 13:44:10.833176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
2022-02-20 13:44:11.292841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-20 13:44:11.292868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 
2022-02-20 13:44:11.292873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N 
2022-02-20 13:44:11.293002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:44:11.293278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:44:11.293519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-20 13:44:11.293928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5342 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5)
2022-02-20 13:44:11.294963: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5583b52b6130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-02-20 13:44:11.294974: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2060, Compute Capability 7.5
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
2022-02-20 13:44:20.425435: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2022-02-20 13:44:20.744285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-02-20 13:44:22.131563: W tensorflow/core/common_runtime/bfc_allocator.cc:311] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.
Train for 10 steps
Epoch 1/4
 1/10 [==>...........................] - ETA: 1:48 - loss: 2.9159 2/10 [=====>........................] - ETA: 49s - loss: 4.0009  3/10 [========>.....................] - ETA: 29s - loss: 5.0575 4/10 [===========>..................] - ETA: 19s - loss: 4.7523 5/10 [==============>...............] - ETA: 12s - loss: 4.5042 6/10 [=================>............] - ETA: 8s - loss: 4.0690  7/10 [====================>.........] - ETA: 5s - loss: 3.7181 8/10 [=======================>......] - ETA: 3s - loss: 3.5984 9/10 [==========================>...] - ETA: 1s - loss: 3.338110/10 [==============================] - 14s 1s/step - loss: 3.3444
Epoch 2/4
 1/10 [==>...........................] - ETA: 1s - loss: 1.4455 2/10 [=====>........................] - ETA: 1s - loss: 1.5891 3/10 [========>.....................] - ETA: 1s - loss: 1.2979 4/10 [===========>..................] - ETA: 1s - loss: 1.2686 5/10 [==============>...............] - ETA: 1s - loss: 1.0406 6/10 [=================>............] - ETA: 0s - loss: 0.9422 7/10 [====================>.........] - ETA: 0s - loss: 0.8666 8/10 [=======================>......] - ETA: 0s - loss: 0.7828 9/10 [==========================>...] - ETA: 0s - loss: 0.809910/10 [==============================] - 2s 208ms/step - loss: 0.7671
Epoch 3/4
 1/10 [==>...........................] - ETA: 1s - loss: 1.0857 2/10 [=====>........................] - ETA: 1s - loss: 2.4411 3/10 [========>.....................] - ETA: 1s - loss: 2.9067 4/10 [===========>..................] - ETA: 1s - loss: 2.6544 5/10 [==============>...............] - ETA: 1s - loss: 2.3458 6/10 [=================>............] - ETA: 0s - loss: 2.0757 7/10 [====================>.........] - ETA: 0s - loss: 2.0728 8/10 [=======================>......] - ETA: 0s - loss: 1.9544 9/10 [==========================>...] - ETA: 0s - loss: 1.911110/10 [==============================] - 2s 217ms/step - loss: 1.8280
Epoch 4/4
 1/10 [==>...........................] - ETA: 1s - loss: 0.7801 2/10 [=====>........................] - ETA: 1s - loss: 0.7663 3/10 [========>.....................] - ETA: 1s - loss: 0.7968 4/10 [===========>..................] - ETA: 1s - loss: 0.6966 5/10 [==============>...............] - ETA: 1s - loss: 0.6320 6/10 [=================>............] - ETA: 0s - loss: 0.9085 7/10 [====================>.........] - ETA: 0s - loss: 0.9989 8/10 [=======================>......] - ETA: 0s - loss: 1.0086 9/10 [==========================>...] - ETA: 0s - loss: 1.004210/10 [==============================] - 2s 209ms/step - loss: 0.9800
2022-02-20 13:44:33.708043: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
training throughput: 35.2457411396123
peak active bytes(MB): 2621.3304290771484
