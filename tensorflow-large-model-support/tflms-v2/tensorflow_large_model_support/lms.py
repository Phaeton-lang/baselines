# Copyright 2019, 2020. IBM All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""LMS
"""
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

import os
import time
from math import floor
from distutils.version import LooseVersion

from tensorflow.python.eager import context
from tensorflow_large_model_support.simulator import Simulator
from tensorflow_large_model_support import topos
from tensorflow_large_model_support import util as ut


class LMS(tf.keras.callbacks.Callback, tf.train.SessionRunHook):
    """LMS class for Large Model Support (LMS).

    The `LMS` object statically modifies a model by swapping its tensors
    to the host so that the model can be trained with the limited memory
    of GPUs.

    Tensors those are generated by forward operations and consumed by
    backward operations are candidates for swapping. The `LMS` object will
    automatically find these tensors.

    Swapping is done by cutting the link between two operations whose
    topological-sort distance between them is greater than a given
    `threshold`, then replacing the link by inserting `identity`
    operations on the host. In theory, this procedure does not have any
    effect on the training convergence as well as inference task.
    """
    def __init__(self,
                 swapout_threshold=-1,
                 swapin_groupby=-1,
                 swapin_ahead=-1,
                 sync_mode=0,
                 serialization=[],
                 serialization_by_size=0,
                 debug=False,
                 debug_level=1,
                 cpu_device="/cpu:0",
                 gpu_device=None):
        """Create an LMS object to edit the graph for supporting large model.

        Args:
          swapout_threshold: if the topological-sort distance between the
            consuming operation and generating operation of a tensor is
            greater (>) than `swapout_threshold`, then trigger swapping the
            tensor. Default `-1` (auto mode).
          swapin_groupby: consuming operations whose distances among them are
            within `swapin_groupby` share the same swap-in operation.
            Default `-1` (auto mode).
          swapin_ahead: lower-bound value for LMS. A tensor will be swapped in
            during the backward phase at least `swapin_ahead` nodes before it
            in the graph. Default `-1` (auto mode).
          sync_mode: whether to do synchronization between data transfer and
            kernel computation or not. Four modes: `0` turn off. `1` sync for
            only swap-out ops. `2` sync for only swap-in ops. `3` sync for both
            swap-out and swap-in ops. Default `0`.
          serialization: serialize operations at the same level in the
            topological sort. This option accepts a list of Python slicing
            string in which each slicing represents level indices in the
            topological sort. E.g. [1, '3:5', 7] means levels 1, 3, 4, 5 and 7
            are serialized. Default `[]` (turn off).
          serialization_by_size: serialize operations in levels of the
            topological sort, if the cumulative memory consumption of
            the level is greater than `serialization_by_size` GiB.
            Default `0` (turn off).
          debug: debug mode for LMS. Default `False`.
          debug_level: debug level for LMS (1 or 2). Default `1`.
          cpu_device: the device we would like swap tensors to.
          gpu_device: the GPU device that this instance of LMS will operate on.
            When models are written in a multi-tower fashion and operations are
            assigned to different GPU devices using semantics like
            `tf.device('/device:GPU:2'), LMS must be instantiated and run
            multiple times, once for each GPU.
        """
        # Validate inputs
        if not isinstance(swapout_threshold, (int)):
            raise ValueError('Unsupported value for swapout_threshold: {}. '
                             'swapout_threshold must be an integer'.format(swapout_threshold))

        if not isinstance(swapin_groupby, (int)):
            raise ValueError('Unsupported value for swapin_groupby: {}. '
                             'swapin_groupby must be an integer'.format(swapin_groupby))

        if not isinstance(swapin_ahead, (int)):
            raise ValueError('Unsupported value for swapin_ahead: {}. '
                             'swapin_ahead must be an integer'.format(swapin_ahead))

        if not isinstance(sync_mode, (int)):
            raise ValueError('Unsupported value for sync_mode: {}. '
                             'sync_mode must be an integer'.format(sync_mode))

        if not (isinstance(serialization_by_size, int) or
                isinstance(serialization_by_size, float)):
            raise ValueError('Unsupported value for serialization_by_size: {}. '
                             'serialization_by_size must be an integer'
                             'or a float.'.format(serialization_by_size))

        if not isinstance(debug_level, (int)):
            raise ValueError('Unsupported value for debug_level: {}. '
                             'debug_level must be an integer'.format(debug_level))

        if swapout_threshold == 0:
            raise ValueError('0 is not a supported value for swapout_threshold.')

        if swapin_ahead == 0:
            raise ValueError('0 is not a supported value for swapin_ahead.')

        valid_sync_mode = {0, 1, 2, 3}
        if sync_mode not in valid_sync_mode:
            raise ValueError('Invalid value for sync_mode {}. '
                             'Valid values are: {}'.format(sync_mode, valid_sync_mode))

        self._swapout_threshold = swapout_threshold
        self._swapin_groupby = swapin_groupby
        self._swapin_ahead = swapin_ahead
        self._sync_mode = sync_mode
        self._serialization = serialization
        self._serialization_by_size = serialization_by_size
        self._cpu_device = cpu_device
        self._gpu_device = gpu_device
        self._debug = debug
        self._debug_level = debug_level

        # True for training, False for inference
        self._is_training = True

        # Keras model or not
        self._is_keras = False

        # optional parameters
        self._excl_output_by_scopes = set()
        self._excl_output_by_types = set()
        self._excl_input_by_scopes = set()
        self._excl_input_by_types = set()
        self._incl_output_by_scopes = set()
        self._incl_output_by_types = set()
        self._incl_input_by_scopes = set()
        self._incl_input_by_types = set()

        # Ops that do not involve in a specific learning mode are inactive.
        # Learning mode is defined by property `is_training`.
        # Never use these ops as a control op. Otherwise, some computations
        # will be ignored.
        self._inactive_ops = None

        # AutoTune
        self._autotune_mode = False
        # Set memory size for auto-tuning, in GiB.
        # otherwise, the actual GPU memory is used.
        self._autotune_gpu_mem = None
        self._autotune_plot = False
        self._autotune_warning = False
        self._max_used_cpu_mem = 0

        # This is used to estimate the size of a tensor.
        self._batch_size = None

        # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/graph_util_impl.py
        self._variable_ops = {
            "Assign",
            "AssignAdd",
            "AssignSub",
            "Queue",
            "ScatterAdd",
            "ScatterSub",
            "ScatterUpdate",
            "TruncatedNormal",
            "Variable",
            "VariableV2",
        }
        # variable ops: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/variable_ops.cc
        self._unused_types = {
            # input data
            'Const', 'Identity', 'Read',
            'Placeholder', 'PlaceholderWithDefault',
            # variable ops
            'VarHandleOp', 'VarIsInitializedOp', 'VariableShape',
            'ReadVariableOp', 'AssignVariableOp',
            'AssignAddVariableOp', 'AssignSubVariableOp'
            'ResourceGather', 'ResourceScatterAdd',
            'ResourceScatterSub', 'ResourceScatterMul',
            'ResourceScatterDiv', 'ResourceScatterMin',
            'ResourceScatterMax', 'ResourceScatterUpdate',
            'ResourceScatterNdUpdate', 'ResourceScatterNdAdd',
            # data filling
            'Range', 'RandomUniform'}

        # a topological sort
        self._topo_sort = None
        # store information to be used to adding control dependencies
        self._swap_ops = []  # [(src_op, swapout_op, swapin_op, dest_op)]
        # control outputs topology
        self._control_outputs = None
        self._version = None

        # a directory to store LMS-related information, such as logs.
        self._lms_dir = ".lms"

    @property
    def is_training(self):
        return self._is_training

    @is_training.setter
    def is_training(self, val):
        self._is_training = val

    @property
    def excl_output_by_scopes(self):
        return self._excl_output_by_scopes

    @excl_output_by_scopes.setter
    def excl_output_by_scopes(self, val):
        self._excl_output_by_scopes = val

    @property
    def excl_output_by_types(self):
        return self._excl_output_by_types

    @excl_output_by_types.setter
    def excl_output_by_types(self, val):
        self._excl_output_by_types = val

    @property
    def excl_input_by_scopes(self):
        return self._excl_input_by_scopes

    @excl_input_by_scopes.setter
    def excl_input_by_scopes(self, val):
        self._excl_input_by_scopes = val

    @property
    def excl_input_by_types(self):
        return self._excl_input_by_types

    @excl_input_by_types.setter
    def excl_input_by_types(self, val):
        self._excl_input_by_types = val

    @property
    def incl_output_by_scopes(self):
        return self._incl_output_by_scopes

    @incl_output_by_scopes.setter
    def incl_output_by_scopes(self, val):
        self._incl_output_by_scopes = val

    @property
    def incl_output_by_types(self):
        return self._incl_output_by_types

    @incl_output_by_types.setter
    def incl_output_by_types(self, val):
        self._incl_output_by_types = val

    @property
    def incl_input_by_scopes(self):
        return self._incl_input_by_scopes

    @incl_input_by_scopes.setter
    def incl_input_by_scopes(self, val):
        self._incl_input_by_scopes = val

    @property
    def incl_input_by_types(self):
        return self._incl_input_by_types

    @incl_input_by_types.setter
    def incl_input_by_types(self, val):
        self._incl_input_by_types = val

    @property
    def batch_size(self):
        return self._batch_size

    @batch_size.setter
    def batch_size(self, val):
        self._batch_size = val

    @property
    def autotune_gpu_mem(self):
        return self._autotune_gpu_mem

    @autotune_gpu_mem.setter
    def autotune_gpu_mem(self, val):
        self._autotune_gpu_mem = val

    @property
    def autotune_plot(self):
        return self._autotune_plot

    @autotune_plot.setter
    def autotune_plot(self, val):
        self._autotune_plot = val

    def run(self, graph=None, keras=False):
        """Edit the graph by adding swapin and swapout ops.

        Swapin and swapout ops are in the host.

        The graph is modified in-place.

        Args:
          graph: the graph we will modify for LMS. This should be the graph of
            user-defined neural network.
        Return:

          a set of added ops.
        """

        if graph is None:
            self._graph = tf.get_default_graph()
            self._log_info("No graph provided. LMS will use the default graph.")
        else:
            self._graph = graph
        self._is_keras = keras
        self._version = self._graph.version

        if self._is_lms_model():
            self._log_info('This model has already been updated with LMS '
                           'swap operations. LMS will not re-process it.')
            return

        self._log_info("Editing model for LMS")
        start_time = time.time()

        # find inactive ops
        self._inactive_ops = self._search_for_inactive_ops(
            self._graph, self._is_training, self._is_keras)

        all_ops = self._graph.get_operations()

        # the size of learning parameters
        learning_params_size = 0
        self._log_info("Number of trainable variables {}".format(
                       len(tf.trainable_variables())), 1)
        for v in tf.trainable_variables():
            size = ut.get_tensor_size(v, self._batch_size)
            self._log_info("Variable {} size {}".format(v, size), 1)
            learning_params_size += size

        # find the largest GPU operation and detect a while-loop
        max_op, max_op_size = None, 0
        has_while = False
        n_edges = 0
        for op in all_ops:
            if (not has_while) and "while/" in op.name:
                has_while = True
            if self._is_valid_op(op) and ut.is_gpu_op(op, self._gpu_device):
                op_size = ut.get_op_size(op, self._batch_size)
                if op_size > max_op_size:
                    max_op_size = op_size
                    max_op = op
            for op1 in ut.fanouts(op):
                n_edges += 1
        self._log_info(
            "The graph has {} vertices and {} edges.".format(
                len(all_ops), n_edges)
        )
        self._log_info("The graph has {} MiB of learning parameters.".format(
            round(learning_params_size/1024/1024, 2)), 0)
        if max_op is not None:
            self._log_info(
                "The largest GPU operation is {}".format(max_op.name) +
                " consuming {} GiB".format(round(max_op_size/1024/1024/1024, 2)))
        if has_while:
            self._log_info("The graph has a while-loop. Please enable GPU-CPU memory" +
                           " swap for the loop by setting swap_memory=True. For more information, visit" +
                           " https://www.tensorflow.org/api_docs/python/tf/while_loop.")
            if self._autotune_enabled():
                raise ValueError("Auto-tuning does not work since the graph has a while-loop." +
                                 " Please set LMS parameters manually.")

        # build a control output topology
        self._control_outputs = ut.build_control_outputs(self._graph)

        # build a topological sort
        self._topo_sort = topos.TOPOS(self._graph)
        self._topo_sort.build()
        self._log_info("Original categorized topological sort has {} levels.".format(
            self._topo_sort.size))
        self._log_level_sizes()
        # exclude cpu ops, ops in other GPUs, ops in other learning mode
        excl_ops = {op for op in all_ops
                    if (ut.is_cpu_op(op) or
                        not ut.is_gpu_op(op, self._gpu_device) or
                        not self._is_valid_op(op))}

        sl_by_size = []
        if self._serialization_by_size:
            sl_by_size = self._get_serialization_levels_by_size(self._serialization_by_size)
            self._log_info("Serialize topological sort levels consuming more than: {} GiB".format(self._serialization_by_size),
                           offset=2)
        self._serialization += sl_by_size

        # serialize the topological sort if enabled
        if self._serialization:
            assign_ops = set()
            for x in ut.get_var_or_handle(tf.global_variables()):
                assign_op = self._graph.get_operation_by_name(
                    "{}/Assign".format(x.name))
                if assign_op is not None:
                    assign_ops.add(assign_op)
            m = max({self._get_level(op) for op in assign_ops}) \
                if len(assign_ops) > 0 else 0
            self._log_info("The maximum level of Assign ops: {}".format(m),
                           offset=2)
            self._log_info("Serialize the topological sort for levels: " +
                           "{}".format(self._serialization),
                           offset=2)
            self._topo_sort.serialize_for(
                self._serialization, min=m, excl_ops=excl_ops)
            self._log_info("New categorized topological sort has {} levels.".format(
                self._topo_sort.size), offset=2)
            self._rebuild_control_outputs(True)  # force rebuilding


        if self._debug and self._debug_level >= 1:
            self._log_info("Operations in each topological sort level:", 1)
            for i in range(0, self._topo_sort.size):
                self._log_info("[{}]: {}".format(
                    i, [(op.name, op.type, op.device)
                        for op in self._get_ops_by_level(i)]), 1)

        # inclusive source ops
        self._incl_src_ops = self._filter_scopes_and_types(
            all_ops, self._incl_output_by_scopes, self._incl_output_by_types)
        # inclusive dest ops
        self._incl_dest_ops = self._filter_scopes_and_types(
            all_ops, self._incl_input_by_scopes, self._incl_input_by_types)
        # exclusive source ops
        self._excl_src_ops = self._filter_scopes_and_types(
            all_ops, self._excl_output_by_scopes,
            self._excl_output_by_types | self._unused_types | self._variable_ops)
        self._excl_src_ops |= excl_ops
        # exclusive dest ops
        self._excl_dest_ops = self._filter_scopes_and_types(
            all_ops, self._excl_input_by_scopes,
            self._excl_input_by_types | self._unused_types | self._variable_ops)
        self._excl_dest_ops |= excl_ops

        if self._autotune_enabled():
            self._search_params()
            if self._swapout_threshold == self._topo_sort.size:
                self._log_info("LMS is not necessary. Turned LMS off.")
                return
            self._validate_parameters()
            self._autotune_mode = True

        self._print_configuration()
        self._log_histogram()  # build a histogram of distance

        # add swapout/swapin ops
        self._rewrite_for_swapping()
        # make sure we are working with the latest control outputs topology
        self._rebuild_control_outputs()
        # add ctrl. dependencies
        if self._swap_ops:
            if self._sync_mode == 0:  # async mode
                self._add_control_dependencies()
            else:  # sync mode
                self._sync_ops()

        # print log information
        swapout_ops = {op[1] for op in self._swap_ops}
        swapin_ops = {op[2] for op in self._swap_ops}
        n_swapout_ops = len(swapout_ops)
        n_swapin_ops = len(swapin_ops)
        swapout_size, swapin_size = 0, 0
        for op in swapin_ops:
            for ts in op.inputs:
                swapin_size += ut.get_tensor_size(ts, self._batch_size)
        for op in swapout_ops:
            for ts in op.outputs:
                swapout_size += ut.get_tensor_size(ts, self._batch_size)

        self._log_info(
            "Added {} operations to the model".format(
                n_swapout_ops + n_swapin_ops) +
            " ({} swap-out operations ({} GiB) and".format(
                n_swapout_ops, round(swapout_size/1024/1024/1024, 2)) +
            " {} swap-in operations ({} GiB))".format(
                n_swapin_ops, round(swapin_size/1024/1024/1024, 2)))
        self._log_info("Editing model for LMS, took: {} ms".format(
            (time.time()-start_time)*1000))

    def _autotune_enabled(self):
        if (self._swapout_threshold > 0 and self._is_swapin_sync()):
            return False
        elif (self._swapout_threshold < 0
              or self._swapin_ahead < 0
              or self._swapin_groupby < 0):
            return True
        return False

    def _model_has_placeholder_inputs(self, graph):
        placeholders = {
            op for op in graph.get_operations()
            if op.type in {'Placeholder', 'PlaceholderWithDefault'}
        }
        for op in placeholders:
            for ts in op.outputs:
                if ts.shape.ndims is not None and ts.shape.ndims > 0:
                    if ts.shape[0].value is None:
                        return True
        return False

    def _rewrite_for_swapping(self):
        """Add swapin and swapout ops for ops that are reachable from `all_ops`.

        Args:
          all_ops: a list of `tf.Operation`
        """
        cand_ops = set(self._graph.get_operations())

        # filter by source operations
        cand_ops = self._filter_by_source_ops(cand_ops)

        for op in cand_ops:
            self._insert_swap_nodes(op)

    def _sync_ops(self):
        """Do synchronization for data transfers between CPU and GPU.
        """
        cpu_ops = {op for op in self._graph.get_operations()
                   if (ut.is_cpu_op(op) and
                       self._is_valid_op(op))}

        # sync for swap-in ops
        if self._is_swapin_sync():
            self._sync_h2d(cpu_ops)
        else:
            self._add_control_dependencies()

        # sync for swap-out ops
        if self._is_swapout_sync():
            self._sync_d2h(cpu_ops)

    def _sync_d2h(self, cpu_ops):
        """Do synchronization for data transfers from device to host.
        """
        def is_device_op(op):
            return (ut.is_gpu_op(op, self._gpu_device) and
                    self._is_valid_op(op))
        for h_op in cpu_ops:
            d_ops = {op for op in ut.fanins(h_op)
                     if is_device_op(op)}
            if len(d_ops) != 1:  # only deal with 1-1 relationship
                continue
            h_op_outs = {op for op in ut.fanouts(h_op) | self._get_control_outputs(h_op)
                         if is_device_op(op)}
            h_op_ins = {op for op in ut.fanins(h_op) | set(h_op.control_inputs)
                        if is_device_op(op)}
            for d_op in d_ops:
                self._log_info("Do synchronization between " +
                               "{} and {}.".format(d_op.name, h_op.name), 1)
                fs = {op for op in ut.fanouts(d_op) | self._get_control_outputs(d_op)
                      if is_device_op(op)}
                # cycles: h_op -> fs_ops -> h_op_ins -> h_op
                fs -= {op1
                       for op1 in fs
                       for op2 in h_op_ins
                       if (self._is_reachable(op1, op2, False, True) or op1 == op2)}
                # duplication
                fs -= h_op_outs
                if fs:
                    for op in fs:
                        self._add_control_inputs(op, h_op)
                else:
                    self._log_info("Could not do synchronization between " +
                                   "{} and {}.".format(d_op.name, h_op.name), 1)

    def _sync_h2d(self, cpu_ops):
        """Do synchronization for data transfers from host to device.
        """
        def is_device_op(op):
            return (ut.is_gpu_op(op, self._gpu_device) and
                    self._is_valid_op(op))
        for h_op in cpu_ops:
            d_ops = {op for op in ut.fanouts(h_op)
                     if is_device_op(op)}
            if len(d_ops) != 1:  # only deal with 1-1 relationship
                continue
            h_op_outs = {op for op in ut.fanouts(h_op) | self._get_control_outputs(h_op)
                         if is_device_op(op)}
            h_op_ins = {op for op in ut.fanins(h_op) | set(h_op.control_inputs)
                        if is_device_op(op)}
            for d_op in d_ops:
                self._log_info("Do synchronization between " +
                               "{} and {}.".format(h_op.name, d_op.name), 1)
                fs = {op for op in ut.fanins(d_op) | set(d_op.control_inputs)
                      if is_device_op(op)}
                if fs:
                    pass
                else:
                    fs = {op
                          for op in self._get_ops_by_level(self._get_level(d_op) - 1)
                          if is_device_op(op)}
                # cycles: h_op -> h_op_outs -> fs_ops -> h_op
                fs -= {op2
                       for op1 in h_op_outs
                       for op2 in fs
                       if (self._is_reachable(op1, op2, False, True) or op1 == op2)}
                # avoid duplication
                fs -= h_op_ins
                if fs:
                    self._add_control_inputs(h_op, fs)
                else:
                    self._log_info("Could not do synchronization between " +
                                   "{} and {}.".format(d_op.name, h_op.name), 1)

    def _groupby(self, ops, limit=5):
        """Group `ops` into groups so that topological distance between
        two consecutive ops in a group is within `limit`.

        Args:
          ops: a set of `tf.Operation`.
          limit: a threshold

        Return:
          A list of sets of `tf.Operation`.
        """
        if limit <= 0:
            return [{op} for op in ops]

        ops_levels = [(op, self._get_level(op)) for op in ops]
        x = sorted([i[1] for i in ops_levels])
        xs = [(i, i) for i in x]

        ys = [xs[0]]
        for i in range(1, len(xs)):
            last = ys[-1]
            curr = xs[i]
            if (curr[0] - last[1] <= limit):
                last = (last[0], curr[1])
                ys[-1] = last
            else:
                ys.append(curr)

        zs = []
        for y in ys:
            gs = set()
            gs = {op[0]
                  for op in ops_levels
                  if (op[1] >= y[0] and op[1] <= y[1])}
            zs.append(gs)
        return zs

    def _insert_swap_nodes(self, src_op):
        """Insert swapin and swapout ops for the given operation into the graph.

        This method does an in-place modification to the graph.

        Args:
          src_op: a `tf.Operation`
        """
        self._log_info("Operation: {}".format(src_op), 2)
        if self._debug and self._debug_level >= 2:
            for ts in src_op.outputs:
                self._log_info("Output tensor: {}".format(ts), 2, 1)
                for op in ts.consumers():
                    self._log_info("Consuming op: {}".format(op.name), 2, 2)

        # obtain candidates
        ts_dests = {}
        for ts in src_op.outputs:
            # filter by tensor shape
            # do not swap 1-dimension or unknown shape tensors.
            ndims = ts.shape.ndims
            if ndims is None or ndims <= 1:
                continue

            # filter by topological distance
            # candidates are ops whose distance to `src_op` is
            # greater than threshold
            cands = self._filter_by_swapout_threshold(
                src_op, ts, set(), self._swapout_threshold)

            # filter by dest operations
            cands = self._filter_by_dest_ops(cands)

            if cands:
                ts_dests[ts] = cands
            else:
                continue

        if ts_dests:
            self._log_info("Operation: {}".format(src_op.name), 1)
        else:
            return

        # rewrite the graph
        for ts in ts_dests:
            # group near candidates by topological distance
            dests_grp = self._groupby(ts_dests[ts], self._swapin_groupby)

            # insert swapout and swap-in ops
            sout, sin_dest = self._insert_swap_nodes_for_ts(
                src_op, ts, dests_grp)

            # keep newly added ops
            for sin, dest in sin_dest:
                self._swap_ops.append((src_op, sout, sin, dest))

    def _insert_swap_nodes_for_ts(self, src_op, ts, targets):
        """Insert swapin and swapout ops for the given tensor into the graph.

        This method does an in-place modification to the graph.

        Args:
          src_op: a `tf.Operation`.
          ts: a `tf.Tensor`, an output of `src_op`.
          targets: a list of sets of consuming ops of `src_op`.

        Return:
          A tuple of a swap-out op and a set of pairs of a consuming op and
          a swap-in op.
        """
        # create a swap_out node
        swapout_op = self._add_swapout(src_op, ts)
        ut.protect_op_from_optimizers(swapout_op)

        # create swap_in nodes
        sin_dest = set()
        for dest_ops in targets:
            # swap_in op
            swapin_op = self._add_swapin(swapout_op, dest_ops, ts)
            ut.protect_op_from_optimizers(swapin_op)
            # for control dependency
            dest_op = self._get_earliest_op(dest_ops)
            sin_dest.add((swapin_op, dest_op))

        return (swapout_op, sin_dest)

    def _add_swapout(self, src_op, ts0):
        """Add a swapout operation to the graph to swap out the output tensor `ts0`
        of the operation `src_op`.

        This method does an in-place modification to the graph.

        Example: the graph before and after this method invoked.
        ```
        Before
          (src_op) -> |ts0| -> (dest_op)

        After:
          (src_op) -> |ts0| -> (swapout_op)
          |ts0| -> (dest_op)
        ```

        Args:
          src_op: a `tf.Operation` that produces the tensor `ts0`.
          ts0: a output `tf.Tensor` of `src_op` being swapped out.

        Return:
          A `tf.Operation` newly added to the graph.
        """
        with tf.device(self._cpu_device):
            swap_out = tf.identity(
                ts0,
                name="lms/swapout_{}".format(
                    ts0.name.replace("/", "_").replace(":", "_")))

        # Connect: src-node -> swap-out
        ut.reroute_input(ts0, swap_out.op.inputs[0], swap_out.op)
        self._log_info("Swap-out: Tensor {} (shape: {})".format(
            ts0.name, ts0.shape), 1, 2)
        self._log_info("Swap-out operation: {}".format(
                swap_out.op.name), 1, 4)
        self._log_info("Connect: {} => {}".format(
            src_op.name, swap_out.op.name), 1, 4)

        return swap_out.op

    def _add_swapin(self, swapout_op, dest_ops, ts0):
        """Add a swapin operation to the graph. The swapin ops reads
        the output tensor of `swapout_op` and passes it to `dest_ops`,
        replacing the input tensors `ts0` of `dest_ops`.

        This method does an in-place modification to the graph.

        Example: the graph before and after this method invoked.
        ```
        Before
          |ts0| -> (swapout_op)
          |ts0| -> (dest_op)

        After:
          |ts0| -> (swapout_op) -> (swapin_op) -> (dest_op)
        ```

        Args:
          swapout_op: a `tf.Operation` that swapped out the tensor `ts0`.
          dest_ops: a set of `tf.Operation` that will consume the output
                    tensor of `swapout_op`.
          ts0: a `tf.Tensor` being the original input tensor of `dest_op`.

        Return:
          A `tf.Operation` newly added to the graph.
        """
        self._log_info("Swap-in: Tensor {} (shape: {})".format(
            ts0.name, ts0.shape), 1, 2)

        with tf.device(self._cpu_device):
            swap_in = tf.identity(
                ts0,
                name="lms/swapin_{}".format(
                    ts0.name.replace("/", "_").replace(":", "_")))

        # Connect: swap_out -> swap_in
        ut.reroute_input(swapout_op.outputs[0],
                         swap_in.op.inputs[0], swap_in.op)
        self._log_info("Swap-in operation: {}".format(
            swap_in.op.name), 1, 4)
        self._log_info("Connect: {} => {}".format(
            swapout_op.name, swap_in.op.name), 1, 4)

        # Connect: swap_in -> dest_ops
        for dest_op in dest_ops:
            ut.reroute_input(swap_in.op.outputs[0], ts0, dest_op)
            self._log_info("Connect: {} => {}".format(
                swap_in.op.name, dest_op.name), 1, 4)

        return swap_in.op

    def _add_control_dependencies(self):
        """Add control dependency operations for all consuming ops.
        """
        if (self._swapin_ahead < 0):
            self._sequential_strategy()
        else:
            # Use the user-defined ahead
            for src, _, sin, dest in self._swap_ops:
                self._add_control_dependency(
                    src, dest, sin, self._swapin_ahead)

    def _sequential_strategy(self):
        """This strategy is to make sure swapins are done in
        a sequential way with respect to the topological levels of
        consuming ops.
        """
        src, sin, dest = 0, 2, 3  # indices
        # sort by destination op's level
        x = sorted(self._swap_ops,
                   key=lambda ops: self._get_level(ops[dest]))

        # a fixed setting for the first swapins.
        x0_dest_level = self._get_level(x[0][dest])
        ahead = 3
        k = 0
        for i in range(0, len(x)):
            if self._get_level(x[i][dest]) == x0_dest_level:
                self._add_control_dependency(
                    x[i][src], x[i][dest], x[i][sin], ahead)
                k = i
            else:
                break

        lb = x0_dest_level
        last_level = lb
        for i in range(k+1, len(x)):
            curr_level = self._get_level(x[i][dest])
            if curr_level != last_level:
                lb = last_level
                last_level = curr_level
            ahead = curr_level - lb
            self._add_control_dependency(
                x[i][src], x[i][dest], x[i][sin], ahead)

    def _add_control_dependency(self, src_op, dest_op, swapin_op, ahead,
                                sync=True):
        """Find and add a control dependency to the graph.

        This method does an in-place modification to the graph.

        Args:
          src_op: a `tf.Operation`.
          dest_op: a `tf.Operation`.
          swapin_op: a `tf.Operation`.
        """
        self._log_info("Find a control dependency for swapping in" +
                       " {} (level {})".format(dest_op.name,
                                               self._get_level(dest_op)), 1)

        re = self._search_by_level(src_op, dest_op, ahead)

        ctrld_op = re[0]
        if ctrld_op:
            self._add_control_inputs(swapin_op, ctrld_op)
        else:
            self._log_info(
                "No control dependency op found for the swap-in " +
                "{}.".format(swapin_op.name), 1)
            if sync:
                # do synchronization
                self._sync_h2d({swapin_op})

    def _search_by_level(self, src_op, dest_op, distance):
        """Find a control dependency operation using topological sort.

        Args:
          src_op: a `tf.Operation` that has a tensor swapped out.
          dest_op: a `tf.Operation` that consumes a tensor swapped in.
          distance: an `integer`. The distance in the topological sort
            between `dest_op` and a candidate for control dependency ops
            must be greater than `distance`.

        Return:
          A tuple of (`tf.Operation`, an `integer`). The first item is
          the control dependency operation that triggers swapping in the input
          tensor of `dest_op`. The second item is the level of the control
          dependency operation in the topological sort.
        """
        result_ops = set()

        range_ub = self._get_level(dest_op) - distance
        range_lb = self._get_level(src_op) + 1
        range_lb = max(range_ub - 10, range_lb)  # try at most 10 levels back

        ctrld_level = -1
        for i in reversed(range(range_lb, range_ub)):
            cands = set()
            for op in self._get_ops_by_level(i):
                # candidates should be able to reach `dest_ops`
                if not self._is_reachable(op, dest_op, False):
                    continue
                # ops must be valid
                if not self._is_valid_op(op):
                    continue
                # ops must be in this GPU
                if not ut.is_gpu_op(op, self._gpu_device):
                    continue
                # ops must be not in a condition scope
                if ("/cond/" in op.name and
                    op.type not in {"Merge", "Switch"}):
                    continue
                # ops must be not in a while-loop scope
                if ("while/" in op.name and
                    op.type not in {"Exit", "Enter"}):
                    continue
                cands.add(op)

            if cands:
                result_ops |= cands
                ctrld_level = i
                break

        if result_ops:
            ctrld_op = next(iter(result_ops))
            return (ctrld_op, ctrld_level)
        else:
            return (None, -1)

    def _validate_parameters(self):
        if self._swapout_threshold == -1:
            raise ValueError(
                'Auto-tuning was unable to find a value for ' +
                'swapout_threshold. Please specify it manually.')

        if not self._is_swapin_sync():
            if self._swapin_ahead == -1:
                raise ValueError(
                    'Auto-tuning was unable to find a value for ' +
                    'swapin_ahead. Please specify it manually.')

            if self._swapin_groupby == -1:
                raise ValueError(
                    'Auto-tuning was unable to find a value for ' +
                    'swapin_groupby. Please specify it manually.')

    def _search_params(self):
        def binary_search(sim, L, R,
                          threshold, ahead, groupby, sync_mode,
                          idx, early_stop=False):
            params = {1: threshold, 2: ahead, 3: groupby}
            warning = False
            while L <= R:
                m = int(floor((L+R)/2))
                old_value = params[idx]
                params[idx] = m
                old_warning = warning
                warning = False
                old_max_used_cpu_mem = self._max_used_cpu_mem
                passed, warning = sim.play(params[1], params[2], params[3], sync_mode)
                self._max_used_cpu_mem = sim._max_used_cpu_mem
                if passed:
                    if early_stop:
                        break
                    L = m + 1
                else:
                    params[idx] = old_value  # restore the previous value
                    warning = old_warning
                    self._max_used_cpu_mem = old_max_used_cpu_mem
                    R = m - 1
            return params[idx], warning

        # This prevents LMS auto tuning from re-running on Keras when the
        # model can train without LMS and  LMS is called in both
        # set_model and set_params from Keras.
        if self._swapout_threshold == self._topo_sort.size:
            return

        self._log_info("Searching values for parameters: " +
                       "swapout_threshold, " +
                       "swapin_ahead, " +
                       "swapin_groupby and sync_mode. ")
        if self._autotune_plot:
            self._log_info("Figures of memory consumption will be generated " +
                           "in directory {}.".format(self._lms_dir))
        mem_ratio_default = 0.9
        if "DDL_OPTIONS" in os.environ:
            mem_ratio_default = 0.8
        mem_ratio = float(os.getenv('TF_LMS_SIMULATOR_MEM_RATIO',
                                    mem_ratio_default))

        if (self._batch_size is None and
            self._model_has_placeholder_inputs(self._graph)):
            raise ValueError(
                "It seems you are feeding data via placeholders. "
                "LMS does not have enough information to auto tune parameters "
                "automatically. Please set the mini-batch size on the "
                "batch_size property of the LMS instance so LMS can tune "
                "parameters automatically or set values for swapout_threshold, "
                "swapin_ahead and swapin_groupby manually.")

        sim = Simulator(self, ratio=mem_ratio, debug_level=1,
                        plot=self._autotune_plot)

        if (self._swapout_threshold < 0
            and self._swapin_ahead < 0
            and self._swapin_groupby < 0):
            # check if we really need LMS or not
            passed, self._autotune_warning = sim.play(self._topo_sort.size, 1, 0, self._sync_mode)
            self._max_used_cpu_mem = sim._max_used_cpu_mem
            if passed:
                self._swapout_threshold = self._topo_sort.size
                return

        found_th, found_ah, found_gr = False, False, False
        # binary search for threshold
        if (self._swapout_threshold < 0):
            sync_mode = self._sync_mode
            found_th = False
            threshold = 1
            ahead = 1 if self._swapin_ahead < 0 else self._swapin_ahead
            groupby = 0 if self._swapin_groupby < 0 else self._swapin_groupby
            while (not found_th):
                passed, self._autotune_warning = sim.play(threshold, ahead, groupby, sync_mode)
                self._max_used_cpu_mem = sim._max_used_cpu_mem
                if passed:
                    self._swapout_threshold, self._autotune_warning = binary_search(
                        sim, 1, self._topo_sort.size,
                        threshold, ahead, groupby, sync_mode, 1)
                    found_th = True
                else:
                    if sync_mode < 3:
                        sync_mode += 1
                    else:
                        return
            self._sync_mode = sync_mode

        # if swapin synchronization is enabled, do not need to search for
        # swapin_ahead and swapin_groupby
        if self._is_swapin_sync():
            return

        # binary search for ahead
        if (self._swapin_ahead < 0):
            threshold = self._swapout_threshold
            ahead = 1
            groupby = 0 if self._swapin_groupby < 0 else self._swapin_groupby
            if not found_th:
                passed, self._autotune_warning = sim.play(threshold, ahead, groupby, self._sync_mode)
                self._max_used_cpu_mem = sim._max_used_cpu_mem
            else:
                passed = True
            if passed:
                self._swapin_ahead, self._autotune_warning = binary_search(
                    sim, 1, self._topo_sort.size,
                    threshold, ahead, groupby, self._sync_mode, 2)
                found_ah = True
            else:
                self._sync_mode = 3
                return

        # binary search for groupby
        if (self._swapin_groupby < 0):
            threshold = self._swapout_threshold
            ahead = self._swapin_ahead
            groupby = 0
            if (not found_th) and (not found_ah):
                passed, self._autotune_warning = sim.play(threshold, ahead, groupby, self._sync_mode)
                self._max_used_cpu_mem = sim._max_used_cpu_mem
            else:
                passed = True
            if passed:
                ok, self._autotune_warning = sim.play(threshold, ahead, self._topo_sort.size,
                              self._sync_mode)
                self._max_used_cpu_mem = sim._max_used_cpu_mem
                if ok:
                    self._swapin_groupby = self._topo_sort.size
                else:
                    self._swapin_groupby, self._autotuning_warning = binary_search(
                        sim, 0, self._topo_sort.size,
                        threshold, ahead, groupby, self._sync_mode, 3,
                        early_stop=True)
                found_gr = True
            else:
                self._sync_mode = 3
                return

    def _is_on_longest_path(self, src_op, dest_op, op):
        """Check if `op` is on the longest path from `src_op` to `dest_op`.
        """
        if not self._is_reachable(src_op, op, True):
            return False
        if not self._is_reachable(op, dest_op, True):
            return False
        return True

    def _filter_scopes_and_types(self, within_ops, scopes, types):
        """Filter out ops that are not in `scopes` and not of `types`.

        Args:
          within_ops: an object convertible to a list of `tf.Operation`.
          scopes: a list of scope path.
          types: a list of tf.DataType.
        Return:
          A set of `tf.Operation`.
        """
        ret_ops = set()
        for scope in scopes:
            ops = {op for op in within_ops
                    if op.name.startswith(scope)}
            if not ops:
                raise ValueError('No operations were found with scope'
                                 ' {}.'.format(scope))
            ret_ops |= ops

        found_types = set()
        type_ops = set()
        for op in within_ops:
            if op.type in types:
                found_types.add(op.type)
                type_ops.add(op)

        # We remove unused types and variabl ops from the input list of
        # `types` because they are constants and not user input. We only want
        # to error if a user provided type is not found.
        user_input_types = types - self._unused_types - self._variable_ops
        missing_types = user_input_types - found_types
        if missing_types:
            raise ValueError('No operations were found with types: '
                             ' {}.'.format(str(missing_types)))

        ret_ops |= type_ops
        return ret_ops

    def _is_reachable(self, src_op, dest_op, via_longest=True,
                      include_control_edges=False):
        """Check whether there exists a path from src_op to dest_op.
        The path's length must be equal to the distance from
        `src_op` to `dest_ops`. In other words, we search for the
        longest path from `src_op` to `dest_op`.

        Args:
          src_op: a starting operation.
          dest_op: a destination operation.

        Return:
          True/False.
        """
        src_level = self._get_level(src_op)
        dest_level = self._get_level(dest_op)

        fanouts = ut.fanouts(src_op)
        if include_control_edges:
            fanouts |= self._get_control_outputs(src_op)
        for l in range(src_level+1, dest_level):
            latest_ops = self._get_ops_by_level(l)
            latest_ops &= fanouts

            if not via_longest:
                if dest_op in latest_ops:
                    return True
            else:
                fanouts = set()

            for op in latest_ops:
                fanouts |= ut.fanouts(op)
                if include_control_edges:
                    fanouts |= self._get_control_outputs(op)

        if dest_op in fanouts:
            return True
        else:
            return False

    def _get_level(self, op):
        """Return the topological level of an operation.

        Args:
          op: a `tf.Operation`.

        Return:
          an integer.
        """
        ret = self._topo_sort.get_level(op)
        if ret is None:
            return -1
        else:
            return ret

    def _get_ops_by_level(self, level):
        """Return a set of ops with the given level.

        Args:
          level: an integer.

        Return:
          a set of `tf.Operation`
        """
        return self._topo_sort.get_ops(level).copy()

    def _filter_by_source_ops(self, cand_ops):
        if self._incl_src_ops:
            # if inclusive mode is enabled,
            # only proceed included ops
            cand_ops &= self._incl_src_ops
        cand_ops = {op for op in cand_ops
                    if op not in self._excl_src_ops}
        return cand_ops

    def _filter_by_dest_ops(self, cand_ops):
        if self._incl_dest_ops:
            # if inclusive mode is enabled,
            # only proceed included ops
            cand_ops &= self._incl_dest_ops
        cand_ops = {op for op in cand_ops
                    if op not in self._excl_dest_ops}
        return cand_ops

    def _filter_by_swapout_threshold(self, src_op, ts, cand_ops, threshold):
        for op in ts.consumers():
            if self._distance(src_op, op) > threshold:
                cand_ops.add(op)
        return cand_ops

    def _log_info(self, message, level=0, offset=0):
        """Log debug information.

        Args:
          message: a formatted string.
          level: an `integer`.
        """
        if level == 0 or (self._debug and self._debug_level >= level):
            # Use tf.logging.info instead of print, since print
            # is not thread safe, which can break tests.
            tf.logging.info("[LMS][{}] ".format(level) +
                            ' '*offset +
                            "{}".format(message))

    def _print_configuration(self):
        """Print configuration information about LMS.
        """
        if self._autotune_mode:
            self._log_info("LMS will use the latest parameter set found by Simulator for the best performance." +
                           " However, if you encounter an out-of-memory error," +
                           " please manually use the previous parameter set found by Simulator.")
        else:
            self._log_info("LMS will use the following parameter set:".format(self._swapout_threshold))
        sync_mode_msg = ""
        if self._sync_mode == 0:
            sync_mode_msg = "Asynchronous memory copy between host and device"
        elif self._sync_mode == 1:
            sync_mode_msg = "Synchronous memory copy from device to host, but asynchronous memory copy from host to device"
        elif self._sync_mode == 2:
            sync_mode_msg = "Asynchronous memory copy from device to host, but synchronous memory copy from host to device"
        elif self._sync_mode == 3:
            sync_mode_msg = "Synchronous memory copy between host and device"
        else:
            pass
        sync_mode_msg = '(' + sync_mode_msg + ')'
        self._log_info("sync_mode: {} {}".format(self._sync_mode, sync_mode_msg), offset=2)

        self._log_info("swapout_threshold: {}".format(self._swapout_threshold), offset=2)

        sync_mode_msg = ""
        if self._sync_mode in {2,3}:
            sync_mode_msg = "(ignored since sync_mode is {})".format(self._sync_mode)
        self._log_info("swapin_ahead: {} {}".format(self._swapin_ahead, sync_mode_msg), offset=2)
        self._log_info("swapin_groupby: {} {}".format(self._swapin_groupby, sync_mode_msg), offset=2)
        if self._autotune_warning:
            max_used_cpu_mem_in_mb = round(self._max_used_cpu_mem/1024/1024)
            if tf.__version__ >= LooseVersion("1.14"):
                self._log_info("Warning: This configuration is likely to hit an out of memory error "
                               + "on the host allocator. It is recommended to increase the values of "
                               + "the TF_GPU_HOST_MEM_LIMIT_IN_MB environment variable "
                               + "to {} or more.".format(max_used_cpu_mem_in_mb), offset=2)
            else:
                self._log_info("Warning: This configuration is likely to hit an out of memory error "
                               + "on the host allocator. It is recommended to increase the values of "
                               + "the TF_CUDA_HOST_MEM_LIMIT_IN_MB enviromnent variable "
                               + "to {} or more.".format(max_used_cpu_mem_in_mb), offset=2)

    def _get_control_outputs(self, op):
        """Return a set of control outputs of an operation.
        """
        if op in self._control_outputs:
            return self._control_outputs[op]
        else:
            return set()

    def _rebuild_control_outputs(self, force=False):
        """Rebuild the control_outputs dictionary if there are ops added
        to the graph.
        """
        if force or (self._version != self._graph.version):
            self._control_outputs = ut.build_control_outputs(self._graph)

    def _update_control_outputs(self, ops, couts):
        """Update control_output sets for operations in `ops`.
        """
        for op in ops:
            if op in self._control_outputs:
                self._control_outputs[op] |= couts
            else:
                self._control_outputs[op] = couts

    def _add_control_inputs(self, op, cops, offset=0):
        """Add control dependencies from `cops` to `op`.

        Args:
          op: a tf.Operation to which the control inputs are added.
          cops: an object convertible to a list of `tf.Operation`.
        Raises:
          TypeError: if op is not a tf.Operation
          ValueError: if any cop in cops is already a control input of op.
        """

        ut.add_control_inputs(op, cops)
        flag = True
        try:
            _ = iter(cops)
        except Exception:  # pylint: disable=broad-except
            flag = False

        if not flag:
            self._update_control_outputs({cops}, {op})
            self._log_info(
                "Control dependency: {} (level {}) => {} (level {})".format(
                    cops.name, self._get_level(cops),
                    op.name, self._get_level(op)),
                1, offset)
        else:
            self._update_control_outputs(cops, {op})
            for cop in cops:
                self._log_info(
                    "Control dependency: " +
                    "{} (level {}) => {} (level {})".format(
                        cop.name, self._get_level(cop),
                        op.name, self._get_level(op)),
                    1, offset)

    def _is_swapin_sync(self):
        return self._sync_mode in {2, 3}

    def _is_swapout_sync(self):
        return self._sync_mode in {1, 3}

    def _log_histogram(self):
        """Log a histogram of distances for edges emanated from `all_ops`.

        Args:
          all_ops: a set of `tf.Operation` to traverse

        Return:
          A dictionary of distance and frequency.
        """
        if not self.autotune_plot:
            return

        if not os.path.exists(self._lms_dir):
            os.makedirs(self._lms_dir)
        hist = {}
        all_ops = self._graph.get_operations()
        import tempfile
        _, f_name = tempfile.mkstemp(dir=self._lms_dir)
        f = open(f_name, "w")
        f.write("#distance\tfrequency\n")
        for op1 in all_ops:
            for op2 in ut.fanouts(op1):
                dist = self._distance(op1, op2)

                if dist in hist:
                    hist[dist] += 1
                else:
                    hist[dist] = 1

        for v in sorted(hist):
            f.write("{}\t{}\n".format(v, hist[v]))
        f.close()
        self._log_info(
            "A histogram of distances was written to {}".format(f_name))
        return hist

    def _distance(self, op1, op2):
        """Return the categorized topological sort distance from `op1` to `op2`.
        """
        return self._get_level(op2) - self._get_level(op1)

    def _search_for_inactive_ops(self, graph, is_training=True, keras=False):
        """Search for ops that do not take part in computation
        in a given learning mode. TFLMS does not deal with these ops.
        """
        def search(x):
            sops = set()
            for front in ut.fanouts(x):
                if front.type in {"Identity"}:
                    sops |= search(front)
                elif front.type in {"Switch"}:
                    sops.add(front)
                else:
                    continue
            return sops

        inactive_ops = set()
        for op in graph.get_operations():
            # ops with learning mode
            cands = {"FusedBatchNorm", "FusedBatchNormGrad"}
            if ((op.type in cands) and (op.get_attr("is_training") != is_training)):
                inactive_ops.add(op)
                continue
            # Ops inside a while-loop except Enter/Exit ops.
            if ("while/" in op.name and op.type not in {"Enter", "Exit"}):
                inactive_ops.add(op)
            # Ops inside a Switch (if-then-else)
            if keras:
                if (op.type in {"PlaceholderWithDefault"} and
                    op.name.endswith("keras_learning_phase")):
                    # find all Switch ops that use the boolean placeholder `keras_learning_phase`
                    switch_ops = search(op)
                    # obtain ops in the inactive branch of Switch
                    for sop in switch_ops:
                        # switch_f: out[0], switch_t: out[1]
                        if is_training:
                            inactive_ops |= set(sop.outputs[0].consumers())
                        else:
                            inactive_ops |= set(sop.outputs[1].consumers())
        # An op is inactive if all of its fan-in ops are inactive
        all_ops = set(graph.get_operations())
        cands = all_ops - inactive_ops
        stop = False
        while (not stop) and cands:
            found = False
            for op in cands:
                if op.type in {"Merge", "Switch"}:
                    continue
                fanins = ut.fanins(op) | set(op.control_inputs)
                if fanins & inactive_ops:
                    # remove known inactive ops first
                    valid = fanins - inactive_ops
                    # remove ops that cannnot be triggered by themselves
                    valid = {x for x in valid
                             if not (x.type in {"Const"} or
                                     "Variable" in op.name)}
                    if len(valid) == 0:  # all fan-in ops are inactive
                        inactive_ops.add(op)
                        found = True
            cands = all_ops - inactive_ops
            if not found:
                stop = True
        return inactive_ops

    def _is_valid_op(self, op):
        """Valid ops are ops that participate in a given learning mode.
        Hence, TFLMS deals with these ops only.
        """
        if len(self._inactive_ops) == 0:
            return True
        else:
            return op not in self._inactive_ops

    def _is_lms_model(self, graph=None):
        """Check if the current model was modified for LMS or not.
        """
        if graph is None:
            graph = self._graph

        for op in graph.get_operations():
            if 'lms/swapout' in op.name:
                src_op = op.inputs[0].op
                if ut.is_gpu_op(src_op, self._gpu_device):
                    return True
        return False

    def _get_earliest_op(self, ops):
        min = self._topo_sort.size
        rop = None
        for op in ops:
            x = self._get_level(op)
            if (x < min and self._is_valid_op(op) and
                ut.is_gpu_op(op, self._gpu_device)):
                min = x
                rop = op
        return rop

    def _get_latest_op(self, ops):
        max = -1
        rop = None
        for op in ops:
            x = self._get_level(op)
            if (x > max and self._is_valid_op(op) and
                ut.is_gpu_op(op, self._gpu_device)):
                max = x
                rop = op
        return rop

    # Implementation of `begin` from tf.train.SessionRunHook
    def begin(self):
        """Implementation of the begin method which is inherited from
        tf.train.SessionRunHook.
        """
        self.run(tf.get_default_graph())

    # Implementation of `set_params` from from tf.keras.callbacks.Callback
    def set_params(self, params):
        self.params = params
        if params.get('batch_size'):
            self._batch_size = self.params['batch_size']
        if (self._autotune_enabled() and not self.batch_size and
                self._model_has_placeholder_inputs(tf.get_default_graph())):
            raise ValueError(
                "TensorFlow Large Model Support (TFLMS) auto tuning is "
                "enabled and the training batch size has not been set on "
                "the LMS instance. Please set the batch_size "
                "parameter or set values for swapout_threshold, "
                "swapin_ahead and swapin_groupby manually.")
        else:
            self._run_for_keras()

    # Implementation of `set_model` from from tf.keras.callbacks.Callback
    def set_model(self, model):
        self._model = model
        if (self._autotune_enabled() and not self.batch_size and
                self._model_has_placeholder_inputs(tf.get_default_graph())):
            # Here log a warning rather than error out because in the fit
            # and fit_generator paths, set_model is called before set_params,
            # and set_params will set the batch_size property when called
            # by model.fit().
            msg = ('TensorFlow Large Model Support (TFLMS) auto tuning is '
                   'enabled and the training batch size has not been set on '
                   'the LMS instance. If this message is received during a '
                   'call to load_model, the use of the loaded model for '
                   'prediction, evaluation, or further training may result in '
                   'out of memory errors. This message can be ignored if it '
                   'is received during a call to model.fit() on a new model.')
            tf.logging.warning(msg)
        else:
            self._run_for_keras()

    def _run_for_keras(self):
        if self._is_lms_model(tf.get_default_graph()):
            return
        if context.executing_eagerly():
            self._log_info('Eager execution mode is not supported with '
                           'TensorFlow Large Model Support.')
            return

        if self._is_training and getattr(self._model, 'train_function', None) is None:
            # This is the tf.keras fit_generator path. As of TensorFlow 1.14.0,
            # all other fit/fit_generator paths in tf.keras and Keras team Keras
            # will have created the train_function before getting to this point.

            # The train_function has not been created, the graph is not
            # "complete" yet because it will not have the optimizer and backward
            # phases in it. We will create the train function now
            # so the model is fully populated for running LMS on it.
            self._log_info('Calling model._make_train_function()', 1)
            self._model._make_train_function()
        self.run(tf.get_default_graph(), True)

    def _log_level_sizes(self):
        """Log the memory consuming sizes of the valid GPU operations in each
        topological level.
        """
        # Only calculate and log level sizes if debug logging is enabled
        if not self._debug:
            return
        # dict of level integer to cumulative size of level
        level_to_size = {}

        for x in range(0, self._topo_sort.size):
            level_to_size[x] = self._get_level_size(x)
        sorted_sizes = sorted(level_to_size.items(), key=lambda item: item[1],
                              reverse=True)
        self._log_info("Cumulative memory consumed by each topological sort "
                       "level:", 1)
        for pair in sorted_sizes:
            self._log_info('Level {} cumulative size {} GiB'.format(pair[0],
                round(pair[1]/1024/1024/1024, 2)), 1)

    def _get_serialization_levels_by_size(self, size):
        """Get a list of topological levels that have valid GPU operations
        which cumulatively consume more memory than the input size.

        Args:
          size: a size in GiB to use as a level filter

        Return:
          A list of topological level integers.
        """
        serialization_levels = []
        for x in range(0, self._topo_sort.size):
            level_size = self._get_level_size(x)
            if level_size/1024/1024/1024 > size:
                serialization_levels.append(x)
        return serialization_levels

    def _get_level_size(self, level):
        ops = self._topo_sort.get_ops(level)
        level_size = 0
        for op in ops:
            if self._is_valid_op(op) and ut.is_gpu_op(op, self._gpu_device):
                level_size += ut.get_op_size(op, self._batch_size)
        return level_size
